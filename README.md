## Requirements

These are the tools required to run this demo. I'm listing the versions I tested with but it's possible that some newer or older versions work as well.

### Necessary

These are strictly necesary to be able to run this demo end-to-end

* [Packer](https://developer.hashicorp.com/packer/downloads) (1.8.6)
* [Vagrant](https://developer.hashicorp.com/vagrant/downloads) (2.3.4)
* [VirtualBox](https://www.virtualbox.org/wiki/Downloads) (6.1.32)
* [Terraform](https://developer.hashicorp.com/terraform/downloads) (1.2.4)
* [Boundary](https://developer.hashicorp.com/boundary/downloads) (CLI 0.13.0, desktop client 1.5.1)
* [Waypoint](https://developer.hashicorp.com/waypoint/downloads) (0.11.1)

### Optional

These are optional, but useful if you want to inspect some intermediate layers

* Any Kubernetes client (e.g. `kubectl`) compatible with the 1.27 API.
* A VNC client. I used [VNC Viewer](https://www.realvnc.com/en/connect/download/viewer/) (6.20.113)
* [Consul](https://developer.hashicorp.com/vagrant/downloads) (1.15.2)
* [Nomad](https://developer.hashicorp.com/nomad/downloads) (1.5.0)
* [Vault](https://developer.hashicorp.com/vault/downloads) (1.14.0)

## Steps

Note: at any step you can inspect the corresponding directory's `Makefile` to see the underlying commands if you prefer to run those directly.

Here's the breakdown of the steps and what it's installed/achieved at each point.

### Building image with Packer

Go to `0-packer/` and run `make build` to kick off the Packer build, followed by `make import` once the build finishes, to import the box into Vagrant.

**What did we achieve here?** You have created a VirtualBox image that you can run in Vagrant.

### Launching Vagrant machines

Go to `1-vagrant/`, run `make start` and wait for the instances to finished being launched and provisioned.

After Vagrant finishes, try running the following commands

Run `consul members` and you should see something like:

```
Node   Address             Status  Type    Build   Protocol  DC   Partition  Segment
node1  192.168.56.78:8301  alive   server  1.15.1  2         dc1  default    <all>
node2  192.168.56.79:8301  alive   server  1.15.1  2         dc1  default    <all>
node3  192.168.56.80:8301  alive   server  1.15.1  2         dc1  default    <all>
```

And then run `nomad node status` and you should see something like:

```
ID        DC   Name   Class   Drain  Eligibility  Status
a0bb127f  dc1  node3  <none>  false  eligible     ready
76a54a68  dc1  node1  <none>  false  eligible     ready
74cd2624  dc1  node2  <none>  false  eligible     ready
```

You can optionally also go to http://localhost:8500 on your browser to see the Consul interface, or to http://localhost:4646 to see the Nomad interface although there won't be anything interesting there yet.

**What did we achieve here?** Now you have a set of three virtual machines launched with Vagrant, each of them running a Consul node forming a 3-node cluster, and also a 3-node Nomad cluster running on top of Consul. There are a series of ports that have been bound on your host machine, pointing at corresponding ports on the first Vagrant machine (`node1`), that will be used through the rest of the demo.

### Deploying k3s cluster with Nomad

Go to `2-k3s/`, run `make apply`, and wait for the `Apply complete!` message. In your terminal run `export KUBECONFIG=~/.nomad-k3s/config` to point to the configuration file that was generated by the new cluster so your Kubernetes client can authenticate with it. After that, you can run `kubectl get nodes` and you should get something like:

```
NAME    STATUS   ROLES                       AGE     VERSION
node1   Ready    control-plane,etcd,master   24s     v1.27.2+k3s1
node2   Ready    control-plane,etcd,master   53s     v1.27.2+k3s1
node3   Ready    control-plane,etcd,master   39s     v1.27.2+k3s1
```

This means the cluster is available. If you're using another client (e.g. [k9s](https://k9scli.io/)) the same `export` command should work as well.

You can go to http://localhost:8500 on your browser to see the Consul interface and verify that the service has been registered through Nomad. You can also go to http://localhost:4646 to see the status of the Nomad tasks.

**What did we achieve here?** We used Terraform to deploy a 3-node k3s cluster distributed across all Nomad nodes. The first node (`k3s-a`) is explicitly deployed on the first node, so the port-forwarding we set up earlier works. Also, k3s-specific Kubernetes config file has been placed in `~/.nomad-k3s/config` to keep it isolated from any existing configuration files.

### Bootstrapping the Kubernetes cluster

Now that we have a Kubernetes cluster, we can deploy Vault and Boundary on it. We'll separate the deployment of these and their configuration at a later step.

Go to `3-k8s-bootstrap/`, run `make apply`, and wait for the `Apply complete!` message.

Inspect the `boundary` namespace in your cluster (e.g. with `kubectl -n boundary get pods`) and you should see something like:

```
NAME                                   READY   STATUS    RESTARTS   AGE
boundary-controller-858d4f84b7-s8n4t   1/1     Running   0          52s
boundary-worker-7b99d44869-dq24j       1/1     Running   0          52s
postgres-postgresql-0                  1/1     Running   0          36s
```

Which means that the Boundary control plane is ready

Now inspect the `vault` namespace (e.g. `kubectl -n vault get pods`) and you'll see the following:

```
NAME      READY   STATUS    RESTARTS   AGE
vault-0   0/1     Running   0          24s
vault-1   0/1     Running   0          24s
vault-2   0/1     Running   0          24s
```

This means the Vault cluster hasn't been initialized and unsealed. To do that, run `make unseal` and you'll see something like:

```
secret/unseal-keys created
Initialized
Key             Value
---             -----
Seal Type       shamir
Initialized     true
Sealed          false
<truncated output>
Key                    Value
---                    -----
Seal Type              shamir
Initialized            true
Sealed                 false
<truncated output>
Key                    Value
---                    -----
Seal Type              shamir
Initialized            true
Sealed                 false
<truncated output>
Unsealed
```

The `Unsealed` message at the end lets you know that the cluster has been unsealed, so if you run `kubectl -n vault get pods` again, you should now see that the pods are ready.

**What did we achieve here?** We used Terraform to deploy a Boundary control plane (one controller and one worker), and a Postgres database for its storage backend on the k3s cluster. We also deployed and initialized a 3-node Vault cluster, distributed across all k3s nodes, using the underlying Consul cluster on the Vagrant nodes as its backend. Both Boundary and Vault are exposed through the ports bound on the host and forwarded to the first Vagrant node.

### Configuring Vault

We'll first configure Vault to allow Boundary to broker secrets for the user.

Go to `4-configure-vault/`, run `make apply`, and wait for the `Apply complete!` message.

**What did we achieve here?** We used Terraform to set some configuration in Vault to allow Boundary to have access to secrets, and we bootstrapped a static secret that we'll use later.

## Configuring Boundary

Now we can set the Boundary configurations that will enable user authentication, secrets brokering, and the discovery of a service we'll deploy later.

Go to `5-configure-boundary/`, run `make apply`, and wait for the `Apply complete!` message.

**What did we achieve here?** We used Terraform to enable the user/pass authentication method, created a demo user, added the necessary Boundary elements to enable service discovery, and created a token on the Vault we deployed earlier, to connect Boundary to Vault as a source of secrets.

### Deploying Waypoint

To complete the 8th piece of the HashiStack, we'll now deploy Waypoint (that we'll later use to deploy another application).

Go to `6-waypoint/`, run `make apply`, and wait for the `Apply complete!` message.

**What did we achieve here?** We have a local Waypoint server running on our k3s cluster, and exposed via a local port that's forwarded to the first Vagrant machine.

### Deploying Kubedoom

Now we'll use Waypoint to deploy an application on the cluster.

Go to `7-kubedoom/`, run `make login init up`, and wait for the `The deploy was successful!` message.

**What did we achieve here?** We have [Kubedoom](https://github.com/storax/kubedoom) running on the cluster, and exposed via a local port that's forwarded to the first Vagrant machine.

### Connecting to Kubedoom

The Kubedoom demo application is exposed via a password-protected VNC endpoint. You don't need what host/port it's exposed on, or what the password is, since you'll proxy access and get credentials brokered by Boundary on our behalf.

* Open your Boundary desktop client, enter `http://localhost:9200` in the `Cluster URL` field, and hit `Continue`. Enter `demo`/`demo1234` as the username and password and click `Sign In`.
* The first thing you should see is the `Targets` tab. Under there there should only be one target called `demo`, and a button that says `Connect` on the right.
* Press the `Connect` button and you'll get a pop up screen with the connection information.
* Copy the host/port indicated under `Proxy URL (Generic TCP)`.
* Enter that into your VNC client. When the connection is getting established you may get a warning saying that the connection is unencrypted, which is expected. Press continue. You'll be prompted for a password, which you'll get in the next step.
* Go back to the Boundary pop up screen, and under `Credentials` you'll see a field called `password`. Copy it and enter it into your VNC client.
* That's it! You should see a familiar Doom interface that you can control with your keyboard. How to play it is beyond the scope of this tutorial, but you can head over to the original Kubedoom page to understand it better.